# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.
bootstrap.servers=node-4.intel.r7g.org:9093,node-5.intel.r7g.org:9093,node-6.intel.r7g.org:9093

# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs
group.id=connect-cluster

# The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will
# need to configure these based on the format they want their data in when loaded from or stored into Kafka
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter

# Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply it to
key.converter.schemas.enable=true
value.converter.schemas.enable=true

# Topic to use for storing offsets. This topic should have many partitions and be replicated and compacted.
offset.storage.topic=__connect_offset
offset.storage.replication.factor=3
offset.storage.partitions=25

# Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated,and compacted topic.
config.storage.topic=__connect_config
config.storage.replication.factor=3

# Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.
status.storage.topic=__connect_status
status.storage.replication.factor=3
# better to keep at 5
status.storage.partitions=5

offset.flush.interval.ms=60000

# List of comma-separated URIs the REST API will listen on. The supported protocols are HTTP and HTTPS.
listeners=https://node-8.intel.r7g.org:8080

# The Hostname & Port that will be given out to other workers to connect to i.e. URLs that are routable from other servers.
# If not set, it uses the value for "listeners" if configured.
rest.advertised.host.name=node-8.intel.r7g.org
rest.advertised.port=8080
rest.advertised.listener=HTTPS

# HTTPS configuration
listeners.https.ssl.client.auth=none
listeners.https.ssl.endpoint.identification.algorithm=
listeners.https.ssl.key.password=insecure
listeners.https.ssl.keystore.location=/etc/kafka/secrets/node-8.intel.r7g.org.jks
listeners.https.ssl.keystore.password=insecure
listeners.https.ssl.keystore.type=JKS
listeners.https.ssl.truststore.location=/etc/kafka/secrets/shared.truststore.jks
listeners.https.ssl.truststore.password=insecure
listeners.https.ssl.truststore.type=JKS

# Set to a list of filesystem paths separated by commas (,) to enable class loading isolation for plugins
# plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors,
plugin.path=/usr/share/java

connector.client.config.override.policy=All

# SSL Configuration
security.protocol=SSL
ssl.endpoint.identification.algorithm=
ssl.key.password=insecure
ssl.keystore.location=/etc/kafka/secrets/node-8.intel.r7g.org.jks
ssl.keystore.password=insecure
ssl.keystore.type=JKS
ssl.truststore.location=/etc/kafka/secrets/shared.truststore.jks
ssl.truststore.password=insecure
ssl.truststore.type=JKS

# SASL Configuration
# uncomment for SASL_SSL , comment for mTLS
# sasl.mechanism=SCRAM-SHA-512
# sasl.jaas.config="org.apache.kafka.common.security.scram.ScramLoginModule required username=\"######\" password=\"#####\";"

# SSL Configuration for source connectors:
#producer.bootstrap.servers=
#producer.security.protocol=SSL
#producer.ssl.truststore.location=/var/private/ssl/kafka.client.truststore.jks
#producer.ssl.truststore.password=test1234
#producer.ssl.keystore.location=/var/private/ssl/kafka.client.keystore.jks
#producer.ssl.keystore.password=test1234
#producer.ssl.key.password=test1234

# SSL Configuration for sink connectors:
#consumer.bootstrap.servers=
#consumer.security.protocol=SSL
#consumer.ssl.truststore.location=/var/private/ssl/kafka.client.truststore.jks
#consumer.ssl.truststore.password=test1234
#consumer.ssl.keystore.location=/var/private/ssl/kafka.client.keystore.jks
#consumer.ssl.keystore.password=test1234
#consumer.ssl.key.password=test1234

# Externalize Secrets
config.providers=file
config.providers.file.class=org.apache.kafka.common.config.provider.FileConfigProvider
# Additional properties added to the connector configuration
# connection.url=${file:/opt/connect-secrets.properties:productsdb-url}
# connection.user=${file:/opt/connect-secrets.properties:productsdb-username}
# connection.password=${file:/opt/connect-secrets.properties:productsdb-password}
# productsdb-url=jdbc:oracle:thin:@myhost:1521:orcl
# productsdb-username=scott
# productsdb-password=my-secret-password
# other-connector-url=jdbc:oracle:thin:@myhost:1521:orcl
# other-connector-username=customers
# other-connector-password=superSecret!