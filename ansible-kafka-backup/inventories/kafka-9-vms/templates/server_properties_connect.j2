# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.
bootstrap.servers={% for host in (groups['combined'] | default([])) + (groups['brokers'] | default([])) %}
{{ hostvars[host]['hostname'] }}:{{ hostvars[host]['port_consumer_mtls_ext'] }}{% if not loop.last %},{% endif %}{% endfor %}


# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs
group.id=connect-cluster-first

# The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will
# need to configure these based on the format they want their data in when loaded from or stored into Kafka
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter

# Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply it to
key.converter.schemas.enable=true
value.converter.schemas.enable=true

# Topic to use for storing offsets. This topic should have many partitions and be replicated and compacted.
offset.storage.topic=_connectors-offsets-custom-topic
offset.storage.replication.factor=3
offset.storage.partitions=25

# Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated,and compacted topic.
config.storage.topic=_connects-configs-custom-topic
config.storage.replication.factor=3

# Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.
status.storage.topic=_connects-status-custom-topic
status.storage.replication.factor=3
# better to keep at 5
status.storage.partitions=5

offset.flush.interval.ms=60000

# List of comma-separated URIs the REST API will listen on. The supported protocols are HTTP and HTTPS.
listeners=http://{{ hostvars[inventory_hostname]['hostname'] }}:{{ hostvars[inventory_hostname]['port_connect_rest_ext'] }}
#,https://{{ hostvars[inventory_hostname]['hostname'] }}:{{ hostvars[inventory_hostname]['port_connect_rest_ext'] }}

# The Hostname & Port that will be given out to other workers to connect to i.e. URLs that are routable from other servers.
# If not set, it uses the value for "listeners" if configured.
#rest.advertised.host.name={{ hostvars[inventory_hostname]['hostname'] }}
#rest.advertised.port={{ hostvars[inventory_hostname]['port_connect_rest_ext'] }}
#rest.advertised.listener=http://{{ hostvars[inventory_hostname]['hostname'] }}:{{ hostvars[inventory_hostname]['port_connect_rest_ext'] }}

# HTTPS configuration
#listeners.https.ssl.client.auth=
#listeners.https.ssl.endpoint.identification.algorithm=
#listeners.https.ssl.key.password=
#listeners.https.ssl.keystore.location=
#listeners.https.ssl.keystore.password=
#listeners.https.ssl.keystore.type=
#listeners.https.ssl.truststore.location=
#listeners.https.ssl.truststore.password=
#listeners.https.ssl.truststore.type=

# Set to a list of filesystem paths separated by commas (,) to enable class loading isolation for plugins
# plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors,
plugin.path=/usr/share/java

connector.client.config.override.policy=All

# SSL Configuration
security.protocol=SSL
ssl.endpoint.identification.algorithm=
ssl.key.password={{ certificate_password }}
ssl.keystore.location=/etc/kafka/secrets/{{ hostvars[inventory_hostname]['hostname'] }}.jks
ssl.keystore.password={{ certificate_password }}
ssl.keystore.type=JKS
ssl.truststore.location=/etc/kafka/secrets/shared.truststore.jks
ssl.truststore.password={{ certificate_password }}
ssl.truststore.type=JKS

# SASL Configuration
# uncomment for SASL_SSL , comment for mTLS
# sasl.mechanism=SCRAM-SHA-512
# sasl.jaas.config="org.apache.kafka.common.security.scram.ScramLoginModule required username=\"######\" password=\"#####\";"

# SSL Configuration for source connectors:
#producer.bootstrap.servers=
#producer.security.protocol=SSL
#producer.ssl.truststore.location=/var/private/ssl/kafka.client.truststore.jks
#producer.ssl.truststore.password=test1234
#producer.ssl.keystore.location=/var/private/ssl/kafka.client.keystore.jks
#producer.ssl.keystore.password=test1234
#producer.ssl.key.password=test1234

# SSL Configuration for sink connectors:
#consumer.bootstrap.servers=
#consumer.security.protocol=SSL
#consumer.ssl.truststore.location=/var/private/ssl/kafka.client.truststore.jks
#consumer.ssl.truststore.password=test1234
#consumer.ssl.keystore.location=/var/private/ssl/kafka.client.keystore.jks
#consumer.ssl.keystore.password=test1234
#consumer.ssl.key.password=test1234

# Externalize Secrets
config.providers=file
config.providers.file.class=org.apache.kafka.common.config.provider.FileConfigProvider
# Additional properties added to the connector configuration
# connection.url=${file:/opt/connect-secrets.properties:productsdb-url}
# connection.user=${file:/opt/connect-secrets.properties:productsdb-username}
# connection.password=${file:/opt/connect-secrets.properties:productsdb-password}
# productsdb-url=jdbc:oracle:thin:@myhost:1521:orcl
# productsdb-username=scott
# productsdb-password=my-secret-password
# other-connector-url=jdbc:oracle:thin:@myhost:1521:orcl
# other-connector-username=customers
# other-connector-password=superSecret!